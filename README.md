# AWS Data Warehouse

## Introduction

In this project, I am going to build an ETL pipeline that extracts their data from AWS S3, stages them in AWS Redshift, and transforms data into a set of dimensional tables. At this point, we are ready to execute SQL statements that create the analytics tables from these staging tables.

## Data description

The dataset consists of two parts Song Dataset and Log Dataset, and The two datasets reside in AWS S3.

Song Dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/), Each file is in JSON format and contains metadata about a song and the artist of that song.

Log Dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Database schema design

First, We need to load data from S3 to staging tables on Redshift. after that, We need to load data from staging tables to analytics tables on Redshift.

**Staging Tables**

staging_songs: info about songs.
staging_events: info about user's actions.

**Fact Table**

songplays: records in log data associated with song plays.


**Dimension Tables**

users: users in the app

songs: songs in music database

artists: artists in music database

time: timestamps of records in songplays broken down into specific units

## Files in the repository


1. **dwh.cfg**: Configuration file containing info about Host and Iam role.
2. **sql_queries.py**: contains SQL queries for creating tables.
3. **create_tables.py**: delete and recreate tables.
6. **etl.py**: contains ETL pipeline.
7. **README.md**: contains descriptions for the project.

## How to run the Python scripts

First, We need to launch a redshift cluster and create an IAM role that has read access to S3. After that, Add redshift database and IAM role info to dwh.cfg.

After data Warehouse Configurations are made, We are now ready to run create_tables.py to drop tables if exists and recreate new tables, Finally run ETL pipeline etl.py.
